{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875a2492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\15143\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\15143\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\15143\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\15143\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\15143\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\15143\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\15143\\anaconda3\\lib\\site-packages (from gensim) (1.9.3)\n",
      "Collecting Cython==0.29.28\n",
      "  Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\15143\\anaconda3\\lib\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\15143\\anaconda3\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Installing collected packages: Cython\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.33\n",
      "    Uninstalling Cython-0.29.33:\n",
      "      Successfully uninstalled Cython-0.29.33\n",
      "Successfully installed Cython-0.29.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\15143\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U scikit-learn\n",
    "#!pip install gensim\n",
    "#!pip install dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71755386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\15143\\anaconda3\\envs\\mlops310523\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\15143\\anaconda3\\envs\\mlops310523\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\15143\\anaconda3\\envs\\mlops310523\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\15143\\anaconda3\\envs\\mlops310523\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\15143\\anaconda3\\envs\\mlops310523\\lib\\site-packages (from scikit-learn) (1.10.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\15143\\anaconda3\\envs\\MLops310523\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4db2c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import docx2txt\n",
    "import pickle\n",
    "import re, os\n",
    "import string\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import warnings\n",
    "import docx2txt\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.svm import SVC\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8729ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "df = pd.read_csv('Resume.csv')\n",
    "# create list of all categories\n",
    "categories = np.sort(df['Category'].unique())\n",
    "# lenght\n",
    "df['length_str'] = df['Resume_str'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa664366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "#Cleaning\n",
    "def clean_text(text):\n",
    "       \n",
    "    text = text.lower() # lowercase text\n",
    "    text = text.replace('\\d+', '') # remove digits\n",
    "    text = re.compile('[/(){}\\[\\]\\|@,;]').sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = re.compile('[^0-9a-z #+_]').sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = ' '.join(word for word in text.split() if word not in set(stopwords.words('english'))) # remove stopwors from text\n",
    "    \n",
    "    \n",
    "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
    "    text = re.sub('#\\S+', '', text)  # remove hashtags\n",
    "    text = re.sub('@\\S+', '  ', text)  # remove mentions\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuations\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ', text) #remplacer tous les caractères non ASCII\n",
    "    text = re.sub('\\s+', ' ', text)  # remove extra whitespace\n",
    "    \n",
    "    # remove non-english characters, punctuation and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) \n",
    "    # tokenize word\n",
    "    text = nltk.tokenize.word_tokenize(text) \n",
    "    # remove stop words\n",
    "    text = [w for w in text if not w in nltk.corpus.stopwords.words('english')]\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(text) for text in text]\n",
    "\n",
    "    return ' '.join(text)\n",
    "\n",
    "def filter_text(text): \n",
    "    text = ' '.join(word for word in text.split() if word not in set(stopwords.words('english')).union(['state','summary', 'city','company', 'name', 'skill'])) # remove stopwors from text\n",
    "    return text\n",
    "\n",
    "#Encode the labels into numeric\n",
    "\n",
    "def set_code(row):\n",
    "    if row[\"Category\"] == \"ACCOUNTANT\": return 1\n",
    "    elif row[\"Category\"] == \"ADVOCATE\": return 2\n",
    "    elif row[\"Category\"] == \"AGRICULTURE\": return 3\n",
    "    elif row[\"Category\"] == \"APPAREL\": return 4\n",
    "    elif row[\"Category\"] == \"ARTS\": return 5\n",
    "    elif row[\"Category\"] == \"AUTOMOBILE\": return 6\n",
    "    elif row[\"Category\"] == \"AVIATION\": return 7\n",
    "    elif row[\"Category\"] == \"BANKING\": return 8\n",
    "    elif row[\"Category\"] == \"BPO\": return 9\n",
    "    elif row[\"Category\"] == \"BUSINESS-DEVELOPMENT\": return 10\n",
    "    elif row[\"Category\"] == \"CHEF\": return 11\n",
    "    elif row[\"Category\"] == \"CONSTRUCTION\": return 12\n",
    "    elif row[\"Category\"] == \"CONSULTANT\": return 13\n",
    "    elif row[\"Category\"] == \"DESIGNER\": return 14\n",
    "    elif row[\"Category\"] == \"DIGITAL-MEDIA\": return 15\n",
    "    elif row[\"Category\"] == \"ENGINEERING\": return 16\n",
    "    elif row[\"Category\"] == \"FINANCE\": return 17\n",
    "    elif row[\"Category\"] == \"HEALTHCARE\": return 18\n",
    "    elif row[\"Category\"] == \"HR\": return 19\n",
    "    elif row[\"Category\"] == \"INFORMATION-TECHNOLOGY\": return 20\n",
    "    elif row[\"Category\"] == \"PUBLIC-RELATIONS\": return 21\n",
    "    elif row[\"Category\"] == \"SALES\": return 22\n",
    "    elif row[\"Category\"] == \"TEACHER\": return 23\n",
    "    elif row[\"Category\"] == \"FITNESS\": return 24\n",
    "    else: return 25  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a7f0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "df = df.assign(target=df.apply(set_code, axis=1))\n",
    "df['Resume_Clean'] = df['Resume_str'].apply(clean_text)\n",
    "df['length_Clean'] = df['Resume_Clean'].str.len()\n",
    "df['Resume_filtered'] = df['Resume_Clean'].apply(lambda w: filter_text(w))\n",
    "df['length_filtered'] = df['Resume_filtered'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a82a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un nouveau DataFrame df1 à partir de df\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4495e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le dossier contenant votre code\n",
    "#folder_path = r\"C:\\Users\\15143\\Desktop\\MLops_Syn_Project\"\n",
    "\n",
    "# Nom du fichier de sauvegarde\n",
    "#file_name = \"df1.csv\"\n",
    "\n",
    "# Chemin complet du fichier de sauvegarde\n",
    "#save_path = folder_path + \"/\" + file_name\n",
    "\n",
    "# Enregistrement du DataFrame en tant que fichier CSV\n",
    "#df1.to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b43366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv('df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64cb1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer toutes les colonnes sauf 'Colonne1' et 'Colonne2'\n",
    "colonnes_a_garder = ['Resume_filtered', 'target']\n",
    "df1 = df1[colonnes_a_garder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0e8a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balance\n",
    "def upsample_classes(data, target):\n",
    "    \n",
    "    lst = list(data[target].unique())\n",
    "    \n",
    "    classes = []\n",
    "    for c in lst:\n",
    "        classes.append(data[data[target]==c])\n",
    "    \n",
    "    length = 0\n",
    "    class_lab = None\n",
    "    for c in classes:\n",
    "        if len(c)>length:\n",
    "            length=len(c)\n",
    "            class_lab = c\n",
    "    class_lab = class_lab[target].unique()[0]\n",
    "    \n",
    "    regroup = pd.concat(classes)\n",
    "    maj_class = regroup[regroup[target]==class_lab]\n",
    "\n",
    "    lst.remove(class_lab)\n",
    "    \n",
    "    new_classes=[]\n",
    "    for i in lst:\n",
    "        new_classes.append(resample(data[data[target]==i],replace=True, n_samples=len(maj_class)))\n",
    "\n",
    "    minority_classes = pd.concat(new_classes)\n",
    "    upsample = pd.concat([regroup[regroup[target]==class_lab],minority_classes])\n",
    "\n",
    "    return upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54fcefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = (upsample_classes(df1,'target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "807658d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "#!pip install --upgrade scikit-learn\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_balanced['Resume_filtered'], df_balanced['target'],stratify=df_balanced['target'], test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd3c9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag Of Words\n",
    "# vectorize text data\n",
    "vectorizer = CountVectorizer()\n",
    "conuntvectorizer_train = vectorizer.fit_transform(X_train).astype(float)\n",
    "conuntvectorizer_test = vectorizer.transform(X_test).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLFlow work Starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30193e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create an experiment in MLFlow and log parameters, metrics and artifacts files like images etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba8b5ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  87.5\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = OneVsRestClassifier(svm.SVC(C=4.0, kernel='linear', degree=3, gamma='auto'))\n",
    "SVM.fit(conuntvectorizer_train, y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(conuntvectorizer_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8fc86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import mlflow\n",
    "#import mlflow.sklearn\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#mlflow.set_tracking_uri(\"https://dagshub.com/herras.adil/MLops_Syn_Project_0906.mlflow\")\n",
    "\n",
    "#os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"herras.adil\"\n",
    "#os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"e7d3465f7a9ab4f2d60b2f026a876a494be74f26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4ce3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run - SVM_2 is logged to Experiment - SVM_classifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\") #uncomment this line if you want to use any database like sqlite as backend storage for model\n",
    "#mlflow.set_tracking_uri(\"https://dagshub.com/herras.adil/MLops_Syn_Project_0906.mlflow\")\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"herras.adil\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"e7d3465f7a9ab4f2d60b2f026a876a494be74f26\"\n",
    "\n",
    "experiment_name = \"SVM_classifier\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "run_name=\"SVM_2\"\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Enregistrer le nom de l'expérience\n",
    "    mlflow.set_tag(\"mlflow.experimentName\", experiment_name)\n",
    "    mlflow.set_tag(\"mlflow.run_name\", run_name)\n",
    "\n",
    "        \n",
    "    # Enregistrer les paramètres de l'exécution\n",
    "    run_params = {\"C\": 4.0, \"kernel\":'linear', \"degree\":3, \"gamma\":'auto'}\n",
    "    mlflow.log_params(run_params)\n",
    "        \n",
    "\n",
    "    # Définir les noms des métriques pour l'exécution\n",
    "    #run_metrics = [\"accuracy\"]\n",
    "    # Calculer et enregistrer les métriques de l'exécution\n",
    "    accuracy=accuracy_score(predictions_SVM, y_test)*100\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy})\n",
    "        \n",
    "    mlflow.sklearn.log_model(SVM, \"SVM_classifier\")\n",
    "        \n",
    "    mlflow.set_tag(\"tag1\", \"SVM_classifier\")\n",
    "    mlflow.set_tags({\"tag2\":\"\", \"tag3\":\"\"})\n",
    "            \n",
    "print('Run - %s is logged to Experiment - %s' %(run_name, experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0c9e653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:  84.72222222222221\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "Random_Forest  = OneVsRestClassifier(RandomForestClassifier(random_state=1,max_depth= 80, min_samples_leaf= 1, min_samples_split= 5, n_estimators= 4000))\n",
    "Random_Forest.fit(conuntvectorizer_train, y_train)\n",
    "model_predictions = Random_Forest.predict(conuntvectorizer_test)\n",
    "print('Random Forest Accuracy: ', accuracy_score(y_test, model_predictions)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2ad73086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run - Random_Forest_classifier_2 is logged to Experiment - Random_Forest_classifier\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\") #uncomment this line if you want to use any database like sqlite as backend storage for model\n",
    "experiment_name = \"Random_Forest_classifier\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "run_name=\"Random_Forest_classifier_2\"\n",
    "    \n",
    "with mlflow.start_run():\n",
    "        \n",
    "       # Enregistrer les paramètres de l'exécution\n",
    "    run_params = {\"random_state\": 1, \"min_samples_leaf\":1, \"max_depth\":80, \"min_samples_split\":5, \"n_estimators\":4000}\n",
    "    mlflow.log_params(run_params)\n",
    "        \n",
    "\n",
    "        # Définir les noms des métriques pour l'exécution\n",
    "    #run_metrics = [\"accuracy\"]\n",
    "        # Calculer et enregistrer les métriques de l'exécution\n",
    "    accuracy=accuracy_score(y_test, model_predictions)*100\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy})\n",
    "        \n",
    "    mlflow.sklearn.log_model(Random_Forest, \"Random_Forest_classifier\")\n",
    "        \n",
    "    mlflow.set_tag(\"tag1\", \"Random_Forest\")\n",
    "    mlflow.set_tags({\"tag2\":\"\", \"tag3\":\"\"})\n",
    "            \n",
    "print('Run - %s is logged to Experiment - %s' %(run_name, experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a40f8603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier Accuracy:  88.19444444444444\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "ovr_classifier  = OneVsRestClassifier(gbc)\n",
    "ovr_classifier.fit(conuntvectorizer_train, y_train)\n",
    "model_predictions = ovr_classifier .predict(conuntvectorizer_test)\n",
    "print('GradientBoostingClassifier Accuracy: ', accuracy_score(y_test, model_predictions)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f3c38f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimator__ccp_alpha: 0.0\n",
      "estimator__criterion: friedman_mse\n",
      "estimator__init: None\n",
      "estimator__learning_rate: 0.1\n",
      "estimator__loss: log_loss\n",
      "estimator__max_depth: 3\n",
      "estimator__max_features: None\n",
      "estimator__max_leaf_nodes: None\n",
      "estimator__min_impurity_decrease: 0.0\n",
      "estimator__min_samples_leaf: 1\n",
      "estimator__min_samples_split: 2\n",
      "estimator__min_weight_fraction_leaf: 0.0\n",
      "estimator__n_estimators: 100\n",
      "estimator__n_iter_no_change: None\n",
      "estimator__random_state: None\n",
      "estimator__subsample: 1.0\n",
      "estimator__tol: 0.0001\n",
      "estimator__validation_fraction: 0.1\n",
      "estimator__verbose: 0\n",
      "estimator__warm_start: False\n",
      "estimator: GradientBoostingClassifier()\n",
      "n_jobs: None\n",
      "verbose: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the training parameters of the classifier\n",
    "params = GradientBoostingClassifier.get_params()\n",
    "\n",
    "# Print the training parameters\n",
    "for param, value in params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2ecf52d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/06 18:25:47 INFO mlflow.tracking.fluent: Experiment with name 'GradientBoostingClassifier_classifier' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run - GradientBoostingClassifier_classifier_2 is logged to Experiment - GradientBoostingClassifier_classifier\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\") #uncomment this line if you want to use any database like sqlite as backend storage for model\n",
    "experiment_name = \"GradientBoostingClassifier_classifier\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "run_name=\"GradientBoostingClassifier_classifier_2\"\n",
    "    \n",
    "with mlflow.start_run():\n",
    "        \n",
    "       # Enregistrer les paramètres de l'exécution\n",
    "    run_params = {\"max_depth\": 3, \"learning_rate\": 0.1, \"n_estimators\":100}\n",
    "    mlflow.log_params(run_params)\n",
    "        \n",
    "\n",
    "        # Définir les noms des métriques pour l'exécution\n",
    "    #run_metrics = [\"accuracy\"]\n",
    "        # Calculer et enregistrer les métriques de l'exécution\n",
    "    accuracy=accuracy_score(y_test, model_predictions)*100\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy})\n",
    "        \n",
    "    mlflow.sklearn.log_model(GradientBoostingClassifier, \"GradientBoostingClassifier_classifier\")\n",
    "        \n",
    "    mlflow.set_tag(\"tag1\", \"GradientBoostingClassifier_classifier\")\n",
    "    mlflow.set_tags({\"tag2\":\"\", \"tag3\":\"\"})\n",
    "            \n",
    "print('Run - %s is logged to Experiment - %s' %(run_name, experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1edab03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoostingClassifier...\n",
      "Best parameters for GradientBoostingClassifier: {'estimator__learning_rate': 0.1, 'estimator__max_depth': 2, 'estimator__n_estimators': 100}\n",
      "GradientBoostingClassifier Accuracy Score on Training Set ->  99.73958333333334\n",
      "Accuracy Score on Test Set for GradientBoostingClassifier: 86.80555555555556\n"
     ]
    }
   ],
   "source": [
    "## GradientBoostingClassifier\n",
    "# liste des modèles que vous souhaitez entraîner\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "models = [\n",
    "    \n",
    "    {\n",
    "        'name': 'GradientBoostingClassifier',\n",
    "        'model': OneVsRestClassifier(GradientBoostingClassifier()),\n",
    "        'params': {\n",
    "            'estimator__learning_rate': [0.05, 0.1],\n",
    "            'estimator__n_estimators': [100, 150],\n",
    "            'estimator__max_depth': [2, 3, 4]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Parcourez la liste des modèles et entraînez-les avec différents hyperparamètres\n",
    "for model in models:\n",
    "    \n",
    "    print(f\"Training {model['name']}...\")\n",
    "    model_tuning = GridSearchCV(model['model'], model['params'], cv=3)\n",
    "    model_tuning.fit(conuntvectorizer_train,y_train)\n",
    "    print(f\"Best parameters for {model['name']}: {model_tuning.best_params_}\")\n",
    "    \n",
    "# Calcul de l'accuracy score sur l'ensemble d'entraînement\n",
    "accuracy = model_tuning.score(conuntvectorizer_train,y_train)\n",
    "\n",
    "print(\"GradientBoostingClassifier Accuracy Score on Training Set -> \", accuracy * 100)\n",
    "\n",
    "# Calcul de l'accuracy score sur l'ensemble de test\n",
    "accuracy_test = model_tuning.score(conuntvectorizer_test, y_test)\n",
    "print(f\"Accuracy Score on Test Set for {model['name']}: {accuracy_test * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "364f0eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score on Validation Set: 88.93709327548807\n",
      "Accuracy Score on Test Set ->  87.67361111111111\n"
     ]
    }
   ],
   "source": [
    "# Diviser les données en ensembles d'entraînement, de validation et de test\n",
    "X_train_1, X_val, y_train_1, y_val = train_test_split(conuntvectorizer_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser le modèle\n",
    "gb = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "\n",
    "# Spécifier les paramètres à tester avec GridSearchCV\n",
    "params = {\n",
    "    'estimator__learning_rate': [0.1],\n",
    "    'estimator__max_depth': [2],\n",
    "    'estimator__n_estimators': [100]\n",
    "}\n",
    "\n",
    "# Initialiser GridSearchCV\n",
    "clf = GridSearchCV(gb, params)\n",
    "\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs paramètres sur l'ensemble d'entraînement\n",
    "clf.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Évaluer les performances sur l'ensemble de validation\n",
    "accuracy_val = clf.score(X_val, y_val)*100\n",
    "print(\"Accuracy Score on Validation Set:\", accuracy_val)\n",
    "\n",
    "# Une fois que vous êtes satisfait des performances sur l'ensemble de validation,\n",
    "# vous pouvez utiliser l'ensemble d'entraînement complet pour entraîner le modèle final\n",
    "\n",
    "# Entraîner le modèle final avec l'ensemble d'entraînement complet\n",
    "clf.fit(conuntvectorizer_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = clf.predict(conuntvectorizer_test)\n",
    "\n",
    "# Calculer l'accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy Score on Test Set -> \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7af99fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run - GradientBoostingClassifier_classifier_1_Optimized_07-06-23 is logged to Experiment - GradientBoostingClassifier_classifier_Optimized_07-06-23\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\") #uncomment this line if you want to use any database like sqlite as backend storage for model\n",
    "experiment_name = \"GradientBoostingClassifier_classifier_Optimized_\" + str(datetime.now().strftime(\"%d-%m-%y\"))\n",
    "mlflow.set_experiment(experiment_name) \n",
    "\n",
    "run_name=\"GradientBoostingClassifier_classifier_1_Optimized_\" + str(datetime.now().strftime(\"%d-%m-%y\"))\n",
    "    \n",
    "with mlflow.start_run():\n",
    "        \n",
    "       # Enregistrer les paramètres de l'exécution\n",
    "    run_params = {\"max_depth\": 2, \"learning_rate\": 0.1, \"n_estimators\":100}\n",
    "    mlflow.log_params(run_params)\n",
    "        \n",
    "\n",
    "        # Définir les noms des métriques pour l'exécution\n",
    "    #run_metrics = [\"accuracy\"]\n",
    "        # Calculer et enregistrer les métriques de l'exécution\n",
    "    accuracy=accuracy_score(y_test, y_pred)*100\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy})\n",
    "        \n",
    "    mlflow.sklearn.log_model(clf, \"GradientBoostingClassifier_classifier_Optimized\")\n",
    "        \n",
    "    mlflow.set_tag(\"tag1\", \"GradientBoostingClassifier_classifier_Optimized\")\n",
    "    mlflow.set_tags({\"tag2\":\"\", \"tag3\":\"\"})\n",
    "            \n",
    "print('Run - %s is logged to Experiment - %s' %(run_name, experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dd8ffba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copie from Mlflow : Predict Data\n",
    "import mlflow\n",
    "logged_model = 'runs:/546fef3f478541ac91750bbe39908caa/GradientBoostingClassifier_classifier_Optimized'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict\n",
    "loaded_model.predict(conuntvectorizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "66b8a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décodage des prédictions\n",
    "# Map the predicted sentiment to the real value\n",
    "reverse_mapping = { 1: \"ACCOUNTANT\", 2: \"ADVOCATE\", 3: \"AGRICULTURE\", 4: \"APPAREL\",\n",
    "    5: \"ARTS\", 6: \"AUTOMOBILE\", 7: \"AVIATION\", 8: \"BANKING\",\n",
    "    9: \"BPO\", 10: \"BUSINESS-DEVELOPMENT\", 11: \"CHEF\", 12: \"CONSTRUCTION\",\n",
    "    13: \"CONSULTANT\", 14: \"DESIGNER\", 15: \"DIGITAL-MEDIA\", 16: \"ENGINEERING\",\n",
    "    17: \"FINANCE\", 18: \"HEALTHCARE\", 19: \"HR\", 20: \"INFORMATION-TECHNOLOGY\",\n",
    "    21: \"PUBLIC-RELATIONS\", 22: \"SALES\", 23: \"TEACHER\", 24: \"FITNESS\", 25: \"UNKNOWN\"}\n",
    "\n",
    "# Make the prediction\n",
    "Resume_Categorie  = loaded_model.predict(conuntvectorizer_test)\n",
    "# Map the predicted sentiment to the category name\n",
    "categories = [reverse_mapping[num] for num in Resume_Categorie]\n",
    "print(\"Predicted Category:\", categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding an MLflow Model to the Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "76c5d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'GradientBoostingClassifier_Optimized'.\n",
      "2023/06/07 08:53:28 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: GradientBoostingClassifier_Optimized, version 1\n",
      "Created version '1' of model 'GradientBoostingClassifier_Optimized'.\n"
     ]
    }
   ],
   "source": [
    "#Method 2\n",
    "import mlflow\n",
    "with mlflow.start_run():\n",
    "    result = mlflow.register_model(\n",
    "        'runs:/546fef3f478541ac91750bbe39908caa/GradientBoostingClassifier_classifier_Optimized',\n",
    "        \"GradientBoostingClassifier_Optimized\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching an MLflow Model from the Model Registry\n",
    "#Fetch a specific model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f1afba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1686142408843, current_stage='Production', description='', last_updated_timestamp=1686142483649, name='GradientBoostingClassifier_Optimized', run_id='546fef3f478541ac91750bbe39908caa', run_link='', source='./artifacts/5/546fef3f478541ac91750bbe39908caa/artifacts/GradientBoostingClassifier_classifier_Optimized', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transitioning an MLflow Model’s Stage\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"GradientBoostingClassifier_Optimized\",\n",
    "    version=1,\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "52abb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction\n",
    "import mlflow.pyfunc\n",
    "model_name = \"GradientBoostingClassifier_Optimized\"\n",
    "version=1\n",
    "model = mlflow.pyfunc.load_model(\n",
    "    model_uri=f\"models:/{model_name}/{version}\")\n",
    "\n",
    "y_pred = model.predict(conuntvectorizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5e62a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction\n",
    "Resume_Categorie  = y_pred\n",
    "# Map the predicted sentiment to the category name\n",
    "categories = [reverse_mapping[num] for num in Resume_Categorie]\n",
    "print(\"Predicted Category:\", categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b07e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc021b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bb484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b68035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead4949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e9eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258d5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8288c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afaee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e918f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction sur de nouvelles données\n",
    "#new_data = docx2txt.process('python-sample-resume.docx')\n",
    "new_data = df.iloc[28].Resume_str\n",
    "\n",
    "# Preprocess the text\n",
    "# Preprocess the text\n",
    "new_data = clean_text(new_data)\n",
    "new_data = filter_text(new_data)\n",
    "\n",
    "# Create a new CountVectorizer with the same vocabulary as the original vectorizer\n",
    "new_vectorizer = CountVectorizer(vocabulary=vectorizer.vocabulary_)\n",
    "\n",
    "# Vectorize the preprocessed text using the new vectorizer\n",
    "features = new_vectorizer.transform([new_data])\n",
    "\n",
    "# Ensure the number of features matches the expected number\n",
    "if features.shape[1] != 33645:\n",
    "    # Pad the features with zeros to match the expected number of features\n",
    "    padding = scipy.sparse.csr_matrix((features.shape[0], 33645 - features.shape[1]))\n",
    "    features = scipy.sparse.hstack([features, padding])\n",
    "\n",
    "# Make the prediction\n",
    "sentiment = clf.predict(features)[0]\n",
    "\n",
    "print(\"Predicted Sentiment:\", sentiment)\n",
    "\n",
    "# Décodage des prédictions\n",
    "# Map the predicted sentiment to the real value\n",
    "reverse_mapping = { 1: \"ACCOUNTANT\", 2: \"ADVOCATE\", 3: \"AGRICULTURE\", 4: \"APPAREL\",\n",
    "    5: \"ARTS\", 6: \"AUTOMOBILE\", 7: \"AVIATION\", 8: \"BANKING\",\n",
    "    9: \"BPO\", 10: \"BUSINESS-DEVELOPMENT\", 11: \"CHEF\", 12: \"CONSTRUCTION\",\n",
    "    13: \"CONSULTANT\", 14: \"DESIGNER\", 15: \"DIGITAL-MEDIA\", 16: \"ENGINEERING\",\n",
    "    17: \"FINANCE\", 18: \"HEALTHCARE\", 19: \"HR\", 20: \"INFORMATION-TECHNOLOGY\",\n",
    "    21: \"PUBLIC-RELATIONS\", 22: \"SALES\", 23: \"TEACHER\", 24: \"FITNESS\", 25: \"UNKNOWN\"}\n",
    "\n",
    "# Make the prediction\n",
    "sentiment = clf.predict(features)[0]\n",
    "# Map the predicted sentiment to the category name\n",
    "predicted_category = reverse_mapping.get(sentiment, \"UNKNOWN\")\n",
    "print(\"Predicted Category:\", predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6066d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7\n"
     ]
    }
   ],
   "source": [
    "#!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773acd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef350f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd66b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a65e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Le processus ne peut pas accéder au fichier car ce fichier est utilisé par un autre processus.\n"
     ]
    }
   ],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d94677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLops310523",
   "language": "python",
   "name": "mlops310523"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
